{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d688d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import TFBertForSequenceClassification, TFBertModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869de9f",
   "metadata": {},
   "source": [
    "# 1. Preparation for data inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c3dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train/val/test data from the data exploration part\n",
    "\n",
    "data_prep=np.load('data_prep.npz',allow_pickle=True)\n",
    "# train_text,train_label=data_prep['train_text'],data_prep['train_label']\n",
    "# val_text,val_label=data_prep['val_text'],data_prep['val_label']\n",
    "test_text,test_label=data_prep['test_text'],data_prep['test_label']\n",
    "voca_set=data_prep['voca_set'].tolist()\n",
    "\n",
    "# define encoder and tokenizer\n",
    "tokenizer=tfds.deprecated.text.Tokenizer()\n",
    "encoder=tfds.deprecated.text.TokenTextEncoder(\n",
    "    voca_set,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b5bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Bert model\n",
    "# ---------------\n",
    "# import train/val/test data from the data exploration part\n",
    "\n",
    "data_prep=np.load('data_prep_bert.npz')\n",
    "\n",
    "# train_reviews=data_prep['train_reviews']\n",
    "# train_segments=data_prep['train_segments']\n",
    "# train_masks=data_prep['train_masks']\n",
    "# train_label_bert=data_prep['train_label']\n",
    "\n",
    "# val_reviews=data_prep['val_reviews']\n",
    "# val_segments=data_prep['val_segments']\n",
    "# val_masks=data_prep['val_masks']\n",
    "# val_label_bert=data_prep['val_label']\n",
    "\n",
    "test_reviews=data_prep['test_reviews']\n",
    "test_segments=data_prep['test_segments']\n",
    "test_masks=data_prep['test_masks']\n",
    "test_label_bert=data_prep['test_label']\n",
    "\n",
    "# function to convert the data into tf.tensor inputs \n",
    "\n",
    "def example_to_features(input_ids,attention_masks,token_type_ids,label):\n",
    "    return {'input_ids':input_ids,\n",
    "            'attention_mask': attention_masks,\n",
    "            'token_type_ids': token_type_ids},label\n",
    "\n",
    "# train_ds=tf.data.Dataset.from_tensor_slices(\n",
    "#     (train_reviews,train_masks,train_segments,train_label_bert)).map(\n",
    "#     example_to_features).shuffle(100).batch(16)\n",
    "\n",
    "# val_ds=tf.data.Dataset.from_tensor_slices(\n",
    "#     (val_reviews,val_masks,val_segments,val_label_bert)).map(\n",
    "#     example_to_features).shuffle(100).batch(16)\n",
    "\n",
    "test_ds=tf.data.Dataset.from_tensor_slices(\n",
    "    (test_reviews,test_masks,test_segments,test_label_bert)).map(\n",
    "    example_to_features).batch(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1270888e",
   "metadata": {},
   "source": [
    "# 2. Predictions from different models and Voting for deciding the final outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12abc41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pred(model_i):\n",
    "    # model_i is the string for model names\n",
    "    # which can be one of the fiollowings\n",
    "#             'BiLSTM',\n",
    "#             'GloVe_Feature_Extraction',\n",
    "#             'GloVe_fine-tuning',\n",
    "#             'Bert_pretrained_head',\n",
    "#             'Bert_custom_head',\n",
    "#             'Bert_custom_head_fine_tune'\n",
    "\n",
    "    # return a 1D array with predicted labels\n",
    "\n",
    "    if model_i=='BiLSTM':\n",
    "\n",
    "        def LSTM_model(voca_size,embedding_dim,rnn_units,batch_size):\n",
    "            model=tf.keras.Sequential([\n",
    "                tf.keras.layers.Embedding(voca_size,embedding_dim,\n",
    "                                          mask_zero=True,\n",
    "                                          batch_input_shape=[batch_size,None]),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Bidirectional(\n",
    "                    tf.keras.layers.LSTM(rnn_units,dropout=0.25)),\n",
    "                tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "            ])\n",
    "            return model\n",
    "\n",
    "        embedding_dim=64\n",
    "        \n",
    "        # length of vocas in chars\n",
    "        voca_size=encoder.vocab_size\n",
    "\n",
    "        # number of RNN units\n",
    "        rnn_units=64\n",
    "\n",
    "        # batch size\n",
    "        batch_size=100\n",
    "\n",
    "        trained_model=LSTM_model(\n",
    "            voca_size=voca_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            rnn_units=rnn_units,\n",
    "            batch_size=batch_size)\n",
    "\n",
    "        # load the model weights\n",
    "        trained_model.load_weights('model_param/BiLSTM.h5')\n",
    "\n",
    "        # use the model to make prediction on the test set\n",
    "\n",
    "        test_outputs=trained_model(test_text)\n",
    "        test_pred=((test_outputs.numpy()>0.5)*1).flatten()\n",
    "\n",
    "        return test_pred\n",
    "\n",
    "    elif 'GloVe' in model_i:\n",
    "\n",
    "        # load the GloVe embeddings\n",
    "\n",
    "        dict_w2v={}\n",
    "\n",
    "        with open(\n",
    "            'GloVe_dict/glove.6B/glove.6B.50d.txt','r',encoding=\"utf8\") as file:\n",
    "            for line in file:\n",
    "                tokens=line.split()\n",
    "                word=tokens[0]\n",
    "                vector=np.array(tokens[1:],dtype=np.float32)\n",
    "\n",
    "                if vector.shape[0]==50:\n",
    "                    dict_w2v[word]=vector\n",
    "                else:\n",
    "                    print('Error with '+word)\n",
    "\n",
    "        # a matrix relating the vocas in our text set to the GloVe dict\n",
    "\n",
    "        embedding_dim=50\n",
    "\n",
    "        embedding_matrix=np.zeros((encoder.vocab_size,embedding_dim))\n",
    "\n",
    "        for word in encoder.tokens:\n",
    "            embedding_vec=dict_w2v.get(word)\n",
    "\n",
    "            if embedding_vec is not None:\n",
    "                tkn_id=encoder.encode(word)[0]\n",
    "                embedding_matrix[tkn_id]=embedding_vec\n",
    "\n",
    "        def LSTM_model(\n",
    "            voca_size,embedding_dim,rnn_units,batch_size,train_emb=False):\n",
    "\n",
    "            model=tf.keras.Sequential([\n",
    "                tf.keras.layers.Embedding(voca_size,embedding_dim,\n",
    "                                          mask_zero=True,\n",
    "                                          weights=[embedding_matrix],\n",
    "                                          trainable=train_emb),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Bidirectional(\n",
    "                    tf.keras.layers.LSTM(rnn_units,dropout=0.25)),\n",
    "                tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "            ])\n",
    "            return model\n",
    "\n",
    "        # length of vocas in chars\n",
    "        voca_size=encoder.vocab_size\n",
    "\n",
    "        # number of RNN units\n",
    "        rnn_units=64\n",
    "\n",
    "        # batch size\n",
    "        batch_size=100\n",
    "\n",
    "        trained_model=LSTM_model(\n",
    "            voca_size=voca_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            rnn_units=rnn_units,\n",
    "            batch_size=batch_size)\n",
    "\n",
    "        # load the model weights\n",
    "\n",
    "        if 'Feature_Extraction' in model_i:\n",
    "\n",
    "            trained_model.load_weights(\n",
    "                'model_param/GloVe_Feature_Extraction.h5')\n",
    "\n",
    "        elif 'fine-tuning' in model_i:\n",
    "\n",
    "            trained_model.load_weights(\n",
    "                'model_param/GloVe_fine-tuning.h5')\n",
    "\n",
    "        else:\n",
    "            print('Error in model name!')\n",
    "            return None\n",
    "\n",
    "        test_outputs=trained_model(test_text)\n",
    "        test_pred=((test_outputs.numpy()>0.5)*1).flatten()\n",
    "\n",
    "        return test_pred\n",
    "\n",
    "\n",
    "    elif 'Bert' in model_i:\n",
    "\n",
    "        if 'pretrained_head' in model_i:\n",
    "\n",
    "            bert=TFBertForSequenceClassification.from_pretrained(\n",
    "                'bert-base-uncased')\n",
    "\n",
    "            bert.load_weights('model_param/Bert_pretrained_bert.h5')\n",
    "            \n",
    "            pred=bert.predict(test_ds)\n",
    "        \n",
    "            return tf.argmax(\n",
    "                tf.nn.softmax(pred.logits,axis=1),axis=1).numpy()\n",
    "\n",
    "        elif 'custom_head' in model_i:\n",
    "\n",
    "            model0=TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "            def head(training=None):\n",
    "\n",
    "                # create the inputs for the BERT model\n",
    "                max_seq_len=512\n",
    "                inp_ids=tf.keras.layers.Input(\n",
    "                    (max_seq_len),dtype=tf.int64,name='input_ids')\n",
    "                att_mask=tf.keras.layers.Input(\n",
    "                    (max_seq_len),dtype=tf.int64,name='attention_mask')\n",
    "                seg_ids=tf.keras.layers.Input(\n",
    "                    (max_seq_len),dtype=tf.int64,name='token_type_ids')\n",
    "\n",
    "                inp_dict={'input_ids':inp_ids,\n",
    "                          'attention_mask':att_mask,\n",
    "                          'token_type_ids':seg_ids}\n",
    "\n",
    "                output=model0(inp_dict) # from the untrained BERT network\n",
    "                x=tf.keras.layers.Dropout(0.2)(output[1],training=training) \n",
    "                x=tf.keras.layers.Dense(200,activation='relu')(x)\n",
    "                x=tf.keras.layers.Dropout(0.2)(x,training=training)\n",
    "                x=tf.keras.layers.Dense(2,activation='sigmoid')(x)\n",
    "\n",
    "                model=tf.keras.models.Model(inputs=inp_dict,outputs=x)\n",
    "\n",
    "                model.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(learning_rate=2E-5),\n",
    "                    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "                return model\n",
    "\n",
    "            bert=head(training=False)\n",
    "\n",
    "            if 'fine_tune' in model_i:\n",
    "                bert.load_weights(\n",
    "                    'model_param/Bert_custom_head_fine_tune.h5')\n",
    "            else:\n",
    "                bert.load_weights(\n",
    "                    'model_param/Bert_custom_head.h5')\n",
    "                \n",
    "            pred=bert.predict(test_ds)\n",
    "            return np.argmax(pred,axis=1)\n",
    "\n",
    "        else:\n",
    "\n",
    "            print('Error in model name!')\n",
    "            return None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d7a51b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: BiLSTM\n",
      "working on: GloVe_fine-tuning\n",
      "working on: Bert_pretrained_head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: Bert_custom_head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "test_acc= 94.24 %\n"
     ]
    }
   ],
   "source": [
    "# model avaialble:[\n",
    "#     'BiLSTM',\n",
    "#     'GloVe_Feature_Extraction',\n",
    "#     'GloVe_fine-tuning',\n",
    "#     'Bert_pretrained_head',\n",
    "#     'Bert_custom_head',\n",
    "#     'Bert_custom_head_fine_tune']\n",
    "\n",
    "# enter the models to be included in the Ensemble Method\n",
    "model_inc=['BiLSTM',\n",
    "           'GloVe_fine-tuning',\n",
    "           'Bert_pretrained_head',\n",
    "           'Bert_custom_head']\n",
    "\n",
    "for i in range(len(model_inc)):\n",
    "    model_i=model_inc[i]\n",
    "    print('working on: '+model_i)\n",
    "    test_outputs=model_pred(model_i)\n",
    "    if i==0:\n",
    "        test_pred=((test_outputs>0.5)*1).flatten()\n",
    "    else:\n",
    "        test_pred+=((test_outputs>0.5)*1).flatten()\n",
    "print('-'*40)\n",
    "\n",
    "# determine the final predicted label by voting\n",
    "pred_label=(test_pred>(len(model_inc)/2))*1\n",
    "test_acc=sum(test_label==pred_label)/len(test_pred)\n",
    "print('test_acc=',test_acc*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b176ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: BiLSTM\n",
      "working on: GloVe_fine-tuning\n",
      "working on: Bert_pretrained_head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "test_acc= 94.15333333333334 %\n"
     ]
    }
   ],
   "source": [
    "# model avaialble:[\n",
    "#     'BiLSTM',\n",
    "#     'GloVe_Feature_Extraction',\n",
    "#     'GloVe_fine-tuning',\n",
    "#     'Bert_pretrained_head',\n",
    "#     'Bert_custom_head',\n",
    "#     'Bert_custom_head_fine_tune']\n",
    "\n",
    "# enter the models to be included in the Ensemble Method\n",
    "model_inc=['BiLSTM',\n",
    "           'GloVe_fine-tuning',\n",
    "           'Bert_pretrained_head']\n",
    "\n",
    "for i in range(len(model_inc)):\n",
    "    model_i=model_inc[i]\n",
    "    print('working on: '+model_i)\n",
    "    test_outputs=model_pred(model_i)\n",
    "    if i==0:\n",
    "        test_pred=((test_outputs>0.5)*1).flatten()\n",
    "    else:\n",
    "        test_pred+=((test_outputs>0.5)*1).flatten()\n",
    "print('-'*40)\n",
    "\n",
    "# determine the final predicted label by voting\n",
    "pred_label=(test_pred>(len(model_inc)/2))*1\n",
    "test_acc=sum(test_label==pred_label)/len(test_pred)\n",
    "print('test_acc=',test_acc*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28c128d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: BiLSTM\n",
      "working on: GloVe_fine-tuning\n",
      "working on: Bert_custom_head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "test_acc= 94.30666666666667 %\n"
     ]
    }
   ],
   "source": [
    "# model avaialble:[\n",
    "#     'BiLSTM',\n",
    "#     'GloVe_Feature_Extraction',\n",
    "#     'GloVe_fine-tuning',\n",
    "#     'Bert_pretrained_head',\n",
    "#     'Bert_custom_head',\n",
    "#     'Bert_custom_head_fine_tune']\n",
    "\n",
    "# enter the models to be included in the Ensemble Method\n",
    "model_inc=['BiLSTM',\n",
    "           'GloVe_fine-tuning',\n",
    "           'Bert_custom_head']\n",
    "\n",
    "for i in range(len(model_inc)):\n",
    "    model_i=model_inc[i]\n",
    "    print('working on: '+model_i)\n",
    "    test_outputs=model_pred(model_i)\n",
    "    if i==0:\n",
    "        test_pred=((test_outputs>0.5)*1).flatten()\n",
    "    else:\n",
    "        test_pred+=((test_outputs>0.5)*1).flatten()\n",
    "print('-'*40)\n",
    "\n",
    "# determine the final predicted label by voting\n",
    "pred_label=(test_pred>(len(model_inc)/2))*1\n",
    "test_acc=sum(test_label==pred_label)/len(test_pred)\n",
    "print('test_acc=',test_acc*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6691580b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: BiLSTM\n",
      "working on: Bert_pretrained_head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: Bert_custom_head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "test_acc= 94.77333333333333 %\n"
     ]
    }
   ],
   "source": [
    "# model avaialble:[\n",
    "#     'BiLSTM',\n",
    "#     'GloVe_Feature_Extraction',\n",
    "#     'GloVe_fine-tuning',\n",
    "#     'Bert_pretrained_head',\n",
    "#     'Bert_custom_head',\n",
    "#     'Bert_custom_head_fine_tune']\n",
    "\n",
    "# enter the models to be included in the Ensemble Method\n",
    "model_inc=['BiLSTM',\n",
    "           'Bert_pretrained_head',\n",
    "           'Bert_custom_head']\n",
    "\n",
    "for i in range(len(model_inc)):\n",
    "    model_i=model_inc[i]\n",
    "    print('working on: '+model_i)\n",
    "    test_outputs=model_pred(model_i)\n",
    "    if i==0:\n",
    "        test_pred=((test_outputs>0.5)*1).flatten()\n",
    "    else:\n",
    "        test_pred+=((test_outputs>0.5)*1).flatten()\n",
    "print('-'*40)\n",
    "\n",
    "# determine the final predicted label by voting\n",
    "pred_label=(test_pred>(len(model_inc)/2))*1\n",
    "test_acc=sum(test_label==pred_label)/len(test_pred)\n",
    "print('test_acc=',test_acc*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9df6a988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: GloVe_fine-tuning\n",
      "working on: Bert_pretrained_head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: Bert_custom_head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "test_acc= 94.82000000000001 %\n"
     ]
    }
   ],
   "source": [
    "# model avaialble:[\n",
    "#     'BiLSTM',\n",
    "#     'GloVe_Feature_Extraction',\n",
    "#     'GloVe_fine-tuning',\n",
    "#     'Bert_pretrained_head',\n",
    "#     'Bert_custom_head',\n",
    "#     'Bert_custom_head_fine_tune']\n",
    "\n",
    "# enter the models to be included in the Ensemble Method\n",
    "model_inc=['GloVe_fine-tuning',\n",
    "           'Bert_pretrained_head',\n",
    "           'Bert_custom_head']\n",
    "\n",
    "for i in range(len(model_inc)):\n",
    "    model_i=model_inc[i]\n",
    "    print('working on: '+model_i)\n",
    "    test_outputs=model_pred(model_i)\n",
    "    if i==0:\n",
    "        test_pred=((test_outputs>0.5)*1).flatten()\n",
    "    else:\n",
    "        test_pred+=((test_outputs>0.5)*1).flatten()\n",
    "print('-'*40)\n",
    "\n",
    "# determine the final predicted label by voting\n",
    "pred_label=(test_pred>(len(model_inc)/2))*1\n",
    "test_acc=sum(test_label==pred_label)/len(test_pred)\n",
    "print('test_acc=',test_acc*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e526c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
